{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: center;\">Fine-Tuning a Faster Convolutional Neural Network for Thermal Signature Classification from Infrared Drone Images in Wildlife Conservation</p>\n",
    "\n",
    "<p style=\"text-align: center;\">Abstract: </p>\n",
    "<p style=\"text-align: center;\"> This tutorial demonstrates how to effectively fine-tune and evaluate a Faster CNN model (namely the foundational ResNet-18 model) using the birdsai dataset of night time infrared images from drones in Southern Africa. The tutorial starts with how to best set up the data for the fine-tuning, then we explore how hyperparameters effect the models accuracy followed by training and evaluating the model with the best found hyperparameters. The resulting model is one that can be used to classify thermal signatures identified by drones at night time as either being that of a humnan or animal, which can support anit-poaching and conservation efforts. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By completing this tutorial, you will be able to:\n",
    "\n",
    "- Understand how to carry out transfer learning for thermal signature classification\n",
    "- Extract thermal signatures from thermal images based off labelled bounding boxes\n",
    "- Understand the impact of hyperparameters on model accuracy (specifically the learning rate and the trained CNN layers)\n",
    "- Evaluate model performance for conservation applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment Setup](#setup)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Thermal Signature Extraction](#signatures)\n",
    "5. [Dataset Creation](#dataset)\n",
    "6. [Model Architecture](#model)\n",
    "7. [Training Process](#training)\n",
    "8. [Hyperparameter Exploration](#hyperparameters)\n",
    "9. [Evaluation](#evaluation)\n",
    "10. [Results and Discussion](#results)\n",
    "11. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction\n",
    "\n",
    "Wildlife conservation efforts are increasingly using drones to capture thermal images (Microavia, 2024) to monitor conservation areas at night for poachers and illegal activity. Being able to automatically distinguish humans, which can be potential poachers, from animals provides a unique challenge due to the small thermal signatures of animals and humans that can be obscured by vegetation.\n",
    "\n",
    "This tutorial applies transfer learning with a pre-trained CNN (ResNet-18) to create an effective classifier for this specialised task. Unlike typical computer vision tutorials that focus on RGB images, our approach addresses thermal imagery's unique characteristics:\n",
    "\n",
    "- Single-channel (grayscale) thermal data instead of 3-channel RGB\n",
    "- Small objects of interest (humans/animals) within larger frames\n",
    "\n",
    "### Why not carry out classification using the entire image?\n",
    "In short: the thermal signatures are too small.\n",
    "\n",
    "We explored fine tuning the ResNet-18 CNN model to classify the entire image as either having a human or animal in it but this did not work as well due to the animals or humans representing small objects within the images. It was deduced that the model would try learn the image background which consitutes a large portion of the image instead of focusing on the small human or animal thermal signaturs in the image. Thus, by focusing only on the thermal signatures of the humans and animals the model can learn the to classify the thermal signatures and not the unimportant background. In the real world if used in  thermal image capturing drones this model would require another model to first detect the thermal signatures to then pass the signatures to this model for classification.\n",
    "\n",
    "Better yet fine tuning a faster RCNN model which can carry out object detection as well as classification would be the best model for deployment in the real world but due to the additional complexity and the desired brevity of this tutorial we just focus on the classification aspect using a faster CNN.\n",
    "\n",
    "### Comparison with Existing Approaches\n",
    "\n",
    "This is the first tutorial of its kind in terms of demonstrating how to carry out transfer learning with a pretrained CNN model using thermal signatures.\n",
    "\n",
    "Looking at other tutorials with similar concepts at a high level of demonstrating how to carry out transfer learning with CNN models on RGB images the following comparison was made:\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| Transfer learning for Image Classification by PyTroch [(Chilamkurthy, 2025)](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) | Simple implementation for beginners. Demonstrates both feature extraction and fine-tuning the final fully connected layer.|Neither explores the effect of differing learning rates or dataset class distribtuions. |\n",
    "| Fine-Tuning for Dog Image classificaton [(Fu, 2023)](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/) | Provides a comparison of training from scratch vs transfer learning. | Neither explores differing hyperparameters or class distributions in the dataset. |\n",
    "| Transfer learning for Dog vs Cat Image Classification [(fchollet, 2023)](https://keras.io/guides/transfer_learning/) | Provides a comprehensive introduction to transfer learning and understanding freezing layers. | Does not intuitively visualise model accuracy in graphs. Neither explores differing hyperparameters or dataset class distributions. |\n",
    "| Fine-Tuning for hand gesture recognition [(Mills, 2024)](https://christianjmills.com/posts/pytorch-train-image-classifier-timm-hf-tutorial/#selecting-a-model) | Provides a comprehensive guide from setting up the python environment to making predictions with the fine-tuned model. Explores available foundational models with emphasis on choosing a task suitable model. | Does not explore the effects of differeing hyperparameters. lacks intuitive graphs for analysing model performance or predictions.  |\n",
    "\n",
    "Our tutorial improves upon existing approaches by:\n",
    "\n",
    "1. **Domain-Suitable Model Selection**: Unlike other tutorials that don't address fine-tuning models that can be suitable for deployment in the real world use case, we specifically choose the ResNet-18 model over bigger foundational models due to it's smaller size and greater efficiency which is more suitale for deployment on thermal imaging drones.\n",
    "\n",
    "2. **Signature-Focused Classification**: We focus on the specific task of thermal signature classification rather than full-frame detection, which is more relevant for real-time wildlife monitoring.\n",
    "\n",
    "3. **Systematic Hyperparameter Experimentation**: We provide a structured comparison of different hyperparameter setups to deduce an optimal model for thermal signature classification.\n",
    "\n",
    "4. **Complete End-to-End Pipeline**: Our tutorial covers the entire process from raw thermal image processing to a deployable model, creating a practical guide for conservation applications.\n",
    "\n",
    "5. **Addresses Data Imbalance**: Our tutorial delves into the data to recognise class distribution imbalances and then creates new datasets with even class distributions to avoid one class from being under represented.\n",
    "\n",
    "6. **Provides intuitive Visualisations**: Unlike other tutorials we visualise model accuracy changes during training and validation. Additionally we display a confusion matrix as well as test set images and the models classifications of these images along with the models confidence score to provide a more visual evaluation of the models accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Define a device which uses the GPU if available, otherwise the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 3. Data Preparation\n",
    "\n",
    "For this tutorial, we're using a subset of the [BirdsAI dataset](https://lila.science/datasets/conservationdrones), which is comprised of thermal images and annotations (labels and bounding boxes) of both humans and a variety of animals.\n",
    "\n",
    "Prerequisite: To run this tutorial yourself you will need the `birdsai_data` folder which is a curated subset of the [BirdsAI dataset](https://lila.science/datasets/conservationdrones) created for this tutorial. This was done by downloading 500 images comprised of both animl and human classes which where then split into a train, validation and test set (we also created a metadata.json file for information on this curated dataset). Please make contact if you would like this specific curated dataset, otherwise you can access the BirdsAI dataset and curate your own sets.\n",
    "\n",
    "First lets explore the dataset and check the image class balance to see whether both classes are represented equally in the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_ROOT = \"birdsai_data\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_ROOT, \"val\")\n",
    "TEST_DIR = os.path.join(DATA_ROOT, \"test\")\n",
    "\n",
    "# Load metadata file which has information on the data sets\n",
    "with open(os.path.join(DATA_ROOT, \"metadata.json\"), \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"Total images: {metadata['actual_dataset_size']}\")\n",
    "print(f\"Training: {metadata['train_images']} images\")\n",
    "print(f\"Validation: {metadata['val_images']} images\")\n",
    "print(f\"Test: {metadata['test_images']} images\")\n",
    "print(f\"Human images: {metadata['human_images']}\")\n",
    "print(f\"Animal images: {metadata['animal_images']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in terms of images across the sets we just about have an even distribution of the two classes. \n",
    "\n",
    "Now lets have a look at some of the anotations of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotation files\n",
    "train_annotations = pd.read_csv(os.path.join(TRAIN_DIR, \"annotations.csv\"))\n",
    "val_annotations = pd.read_csv(os.path.join(VAL_DIR, \"annotations.csv\"))\n",
    "test_annotations = pd.read_csv(os.path.join(TEST_DIR, \"annotations.csv\"))\n",
    "\n",
    "# Display first few rows of training annotations\n",
    "train_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By displaying some of the annotation data it is now evident that for the same frame or image there can be multiple objects. This is the case as each image has at least one animal or human thermal signature. Since we are classifying the thermal signatures we should delve into how many unique thermal signatures there are across both classes in all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring our Thermal Signature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance in each split's annotations\n",
    "def check_class_balance(annotations, split_name):\n",
    "    class_counts = annotations['class'].value_counts()\n",
    "    human_count = class_counts.get(1, 0)\n",
    "    animal_count = class_counts.get(0, 0)\n",
    "    total = human_count + animal_count\n",
    "    \n",
    "    print(f\"{split_name} Set Class Distribution:\")\n",
    "    print(f\"Human (class 1): {human_count} annotations ({human_count/total:.1%})\")\n",
    "    print(f\"Animal (class 0): {animal_count} annotations ({animal_count/total:.1%})\")\n",
    "    print(f\"Total: {total} annotations\\n\")\n",
    "\n",
    "# Check class balance for each split\n",
    "check_class_balance(train_annotations, \"Training\")\n",
    "check_class_balance(val_annotations, \"Validation\")\n",
    "check_class_balance(test_annotations, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delving into the number of annotations for each human and animal object we see that the human thermal signatures are under represented across the images. This is not a surprise as animals tend to be in large groups. To ensure that we train a model on an even amount of data across both classes we will need to create new train, validation and test sets with even representaions of both animal and human thermal signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the distribution of object sizes in our dataset. This will help us understand what sizes of thermal signatures we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate object sizes\n",
    "train_annotations['area'] = train_annotations['w'] * train_annotations['h']\n",
    "\n",
    "# Create boxplot of object sizes by class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='class', y='area', data=train_annotations)\n",
    "plt.title('Distribution of Object Sizes by Class')\n",
    "plt.xlabel('Class (0=Animal, 1=Human)')\n",
    "plt.ylabel('Area (pixels²)')\n",
    "plt.yscale('log')  # Log scale for better visualisation\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics of object sizes\n",
    "size_stats = train_annotations.groupby('class')['area'].describe()\n",
    "print(\"Object Size Statistics:\")\n",
    "print(size_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the object sizes differ between the two classes as animals (especially elephants which are present in some of the images) tend to be larger than humans. Now that we are aware of the object sizes we can choose a standard area size to extract from the images to represent the thermal signatures across the two classes which is important for passing standardised image sizes to the CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"signatures\"></a>\n",
    "## 4. Thermal Signature Extraction\n",
    "\n",
    "Now, let's visualise some examples from our dataset with bounding boxes to show what the thermal images and signatures look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_thermal_image_with_bbox(split_dir, frame_number, annotations):\n",
    "    \"\"\"Visualise thermal image with bounding boxes.\"\"\"\n",
    "    # Load image\n",
    "    img_path = os.path.join(split_dir, \"images\", f\"image_{frame_number:06d}.jpg\")\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get annotations for this frame\n",
    "    frame_annos = annotations[annotations['frame_number'] == frame_number]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    \n",
    "    # Show original image with bounding boxes\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Thermal Image with Bounding Boxes')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Colours for different classes\n",
    "    colors = {0: 'red', 1: 'green'}\n",
    "    class_names = {0: 'Animal', 1: 'Human'}\n",
    "    \n",
    "    # Get first annotation to show as extracted signature\n",
    "    if len(frame_annos) > 0:\n",
    "        first_anno = frame_annos.iloc[0]\n",
    "        x, y, w, h = int(first_anno['x']), int(first_anno['y']), int(first_anno['w']), int(first_anno['h'])\n",
    "        class_id = int(first_anno['class'])\n",
    "        \n",
    "        # Extract and show the thermal signature\n",
    "        signature = img[y:y+h, x:x+w]\n",
    "        ax2.imshow(signature)\n",
    "        ax2.set_title(f'Extracted Thermal Signature ({class_names[class_id]})')\n",
    "        ax2.axis('off')\n",
    "    \n",
    "    # Draw all bounding boxes\n",
    "    for _, anno in frame_annos.iterrows():\n",
    "        x, y, w, h = int(anno['x']), int(anno['y']), int(anno['w']), int(anno['h'])\n",
    "        class_id = int(anno['class'])\n",
    "        \n",
    "        rect = plt.Rectangle((x, y), w, h, linewidth=2, \n",
    "                           edgecolor=colors[class_id], facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(x, y-10, class_names[class_id], color=colors[class_id], fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return img, frame_annos\n",
    "\n",
    "# Select an example from training set\n",
    "# Find one frame with a human\n",
    "human_example = train_annotations[train_annotations['class'] == 1]['frame_number'].iloc[0]\n",
    "# Find one frame with an animal\n",
    "animal_example = train_annotations[train_annotations['class'] == 0]['frame_number'].iloc[0]\n",
    "\n",
    "# Visualise examples\n",
    "print(\"Example with Human:\")\n",
    "img_human, annos_human = visualise_thermal_image_with_bbox(TRAIN_DIR, human_example, train_annotations)\n",
    "\n",
    "print(\"\\nExample with Animal:\")\n",
    "img_animal, annos_animal = visualise_thermal_image_with_bbox(TRAIN_DIR, animal_example, train_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Thermal Signatures\n",
    "\n",
    "Based on the bounding box annotations, we'll extract the thermal signatures (the regions containing humans or animals) and preprocess them for our classifier. We'll resize them to a standard size, normalise the pixel values, and create augmentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First lets define a function which will locate the object using the bounding box's `x` and `y` pixel locations and the `w` (width) and `h` (height) values, thereafter resizing the area of interest to a standard size using the maximum mean object size we identified the earlier step (`240` squared pixels in this case).\n",
    "\n",
    "Special Note: `cv2.imread()` loads images in BGR format (3 channels) by default so we convert it back to grayscale (1 channel) where later we will repeat this single channel into 3 identical channels as the foundational CNN model works with `3 channel inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_thermal_signature(img_path, x, y, w, h, target_size=(240, 240)):\n",
    "    \"\"\"Extract thermal signature from an image given bounding box coordinates.\"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Extract signature using bounding box\n",
    "    signature = gray[y:y+h, x:x+w]\n",
    "    \n",
    "    # Resize to target size\n",
    "    signature_resized = cv2.resize(signature, target_size)\n",
    "    \n",
    "    # Convert to PIL (Python Imaging Library) image for PyTorch transformations\n",
    "    signature_pil = Image.fromarray(signature_resized)\n",
    "    \n",
    "    return signature_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lets test this function by visualising some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_extracted_signatures(split_dir, annotations, number_of_examples=3):\n",
    "    # Get some human examples\n",
    "    human_frames = annotations[annotations['class'] == 1].sample(number_of_examples)\n",
    "    # Get some animal examples\n",
    "    animal_frames = annotations[annotations['class'] == 0].sample(number_of_examples)\n",
    "    \n",
    "    fig,axes = plt.subplots(2, number_of_examples, figsize=(15, 8))\n",
    "    \n",
    "    # Process human examples\n",
    "    for i, (_, anno) in enumerate(human_frames.iterrows()):\n",
    "        frame_num = anno['frame_number']\n",
    "        img_path = os.path.join(split_dir, \"images\", f\"image_{int(frame_num):06d}.jpg\")\n",
    "        x, y, w, h = int(anno['x']), int(anno['y']), int(anno['w']), int(anno['h'])\n",
    "        \n",
    "        signature = extract_thermal_signature(img_path, x, y, w, h)\n",
    "        axes[0, i].imshow(signature, cmap='gray')\n",
    "        axes[0, i].set_title(f\"Human {i+1}\")\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Process animal examples\n",
    "    for i, (_, anno) in enumerate(animal_frames.iterrows()):\n",
    "        frame_num = anno['frame_number']\n",
    "        img_path = os.path.join(split_dir, \"images\", f\"image_{int(frame_num):06d}.jpg\")\n",
    "        x, y, w, h = int(anno['x']), int(anno['y']), int(anno['w']), int(anno['h'])\n",
    "        \n",
    "        signature = extract_thermal_signature(img_path, x, y, w, h)\n",
    "        axes[1, i].imshow(signature, cmap='gray')\n",
    "        axes[1, i].set_title(f\"Animal {i+1}\")\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show examples of extracted signatures\n",
    "show_extracted_signatures(TRAIN_DIR, train_annotations, number_of_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key noticable difference between the human and animal thermal signatures is the shape and orientation. We will randomly rotate the images in our training data to create a more diverse thermal signature orientations to help our model generalise better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "## 5. Dataset Creation\n",
    "\n",
    "Now, we'll create a PyTorch dataset class to load and preprocess our thermal signatures. This will handle the extraction of signatures from the original images and apply appropriate transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThermalSignatureDataset(Dataset):\n",
    "    def __init__(self, annotations, base_dir, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anno = self.annotations.iloc[idx]\n",
    "        \n",
    "        # Get image path\n",
    "        frame_num = anno['frame_number']\n",
    "        img_path = os.path.join(self.base_dir, \"images\", f\"image_{int(frame_num):06d}.jpg\")\n",
    "        \n",
    "        # Extract signature\n",
    "        x, y, w, h = int(anno['x']), int(anno['y']), int(anno['w']), int(anno['h'])\n",
    "        signature = extract_thermal_signature(img_path, x, y, w, h)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            signature = self.transform(signature)\n",
    "        \n",
    "        # Get label\n",
    "        label = int(anno['class'])\n",
    "        \n",
    "        return signature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define transformations\n",
    "\n",
    "We augment the training data thermal signatures by applying random rotations to create more `diverse orientations` of the thermal signatures to help our model learn more generalisable features.\n",
    "Additionally due to the foundational CNN model being trained on RGB images (3 channels), we need to format our images to have `3 channels` by tripling the single channel we to ensure our images can be passed to the model. We do this by utilising the `Repeat()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, we'll include data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229]),  # Single channel normalisation\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Repeat grayscale to 3 channels for the model which accepts 3 channel inputs\n",
    "])\n",
    "\n",
    "# For validation and testing, we'll just resize and normalise\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We need to create balanced train, validation and test sets in terms of thermal signature distribution across the two classes. We define a function below to create sets of images which have an even distribution of the number annotations for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_datasets(train_annotations, val_annotations, test_annotations):\n",
    "    # For training set\n",
    "    train_humans = train_annotations[train_annotations['class'] == 1]\n",
    "    train_animals = train_annotations[train_annotations['class'] == 0]\n",
    "    \n",
    "    # Calculate how many samples to use (use the smaller class count)\n",
    "    train_sample_size = min(len(train_humans), len(train_animals))\n",
    "    \n",
    "    # Sample equal numbers from each class\n",
    "    if len(train_humans) > train_sample_size:\n",
    "        train_humans = train_humans.sample(train_sample_size, random_state=42)\n",
    "    if len(train_animals) > train_sample_size:\n",
    "        train_animals = train_animals.sample(train_sample_size, random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_train = pd.concat([train_humans, train_animals]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Repeat for validation and test sets\n",
    "    val_humans = val_annotations[val_annotations['class'] == 1]\n",
    "    val_animals = val_annotations[val_annotations['class'] == 0]\n",
    "    val_sample_size = min(len(val_humans), len(val_animals))\n",
    "    \n",
    "    if len(val_humans) > val_sample_size:\n",
    "        val_humans = val_humans.sample(val_sample_size, random_state=42)\n",
    "    if len(val_animals) > val_sample_size:\n",
    "        val_animals = val_animals.sample(val_sample_size, random_state=42)\n",
    "    \n",
    "    balanced_val = pd.concat([val_humans, val_animals]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    test_humans = test_annotations[test_annotations['class'] == 1]\n",
    "    test_animals = test_annotations[test_annotations['class'] == 0]\n",
    "    test_sample_size = min(len(test_humans), len(test_animals))\n",
    "    \n",
    "    if len(test_humans) > test_sample_size:\n",
    "        test_humans = test_humans.sample(test_sample_size, random_state=42)\n",
    "    if len(test_animals) > test_sample_size:\n",
    "        test_animals = test_animals.sample(test_sample_size, random_state=42)\n",
    "    \n",
    "    balanced_test = pd.concat([test_humans, test_animals]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Display class balance information\n",
    "    print(\"Original vs Balanced Datasets:\")\n",
    "    print(f\"Training: {len(train_annotations)} → {len(balanced_train)} annotations\")\n",
    "    print(f\"  Original: {sum(train_annotations['class'] == 1)} humans, {sum(train_annotations['class'] == 0)} animals\")\n",
    "    print(f\"  Balanced: {sum(balanced_train['class'] == 1)} humans, {sum(balanced_train['class'] == 0)} animals\")\n",
    "    \n",
    "    print(f\"Validation: {len(val_annotations)} → {len(balanced_val)} annotations\")\n",
    "    print(f\"  Original: {sum(val_annotations['class'] == 1)} humans, {sum(val_annotations['class'] == 0)} animals\")\n",
    "    print(f\"  Balanced: {sum(balanced_val['class'] == 1)} humans, {sum(balanced_val['class'] == 0)} animals\")\n",
    "    \n",
    "    print(f\"Test: {len(test_annotations)} → {len(balanced_test)} annotations\")\n",
    "    print(f\"  Original: {sum(test_annotations['class'] == 1)} humans, {sum(test_annotations['class'] == 0)} animals\") \n",
    "    print(f\"  Balanced: {sum(balanced_test['class'] == 1)} humans, {sum(balanced_test['class'] == 0)} animals\")\n",
    "    \n",
    "    return balanced_train, balanced_val, balanced_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the datasets of images with even sums of class annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train, balanced_val, balanced_test = create_balanced_datasets(\n",
    "    train_annotations, val_annotations, test_annotations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Creat the final datasets with the extracted and processed thermal signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ThermalSignatureDataset(balanced_train, TRAIN_DIR, transform=train_transform)\n",
    "val_dataset = ThermalSignatureDataset(balanced_val, VAL_DIR, transform=val_transform)\n",
    "test_dataset = ThermalSignatureDataset(balanced_test, TEST_DIR, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created our datasets with even class distributions ready for model training, evaluation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 6. Model Architecture\n",
    "\n",
    "For our classification task, we'll use transfer learning with a pre-trained ResNet-18 CNN model supported by PyTorch for our binary thermal signature classification task (human vs animal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning Approaches\n",
    "\n",
    "For our model, two approaches of transfer learning are explored:\n",
    "\n",
    "1. **Frozen Backbone**: \n",
    "   - In this approach, we keep the pre-trained weights of the convolutional layers (the \"backbone\") fixed\n",
    "   - We only train the new classification layers we add\n",
    "   - This approach is faster to train as minimal layers are trained\n",
    "   - Performs well when the images we input to the model are similar to the images that were used to train the foundational model (unlikely in our case).\n",
    "\n",
    "2. **Fine-tuned Backbone**:\n",
    "   - We update all layers of the backbone during training on our thermal signatures which allows the model to adapt to our thermal signatures\n",
    "   - Requires more training time\n",
    "\n",
    "The ideal approach depends on the specific dataset and task. We'll compare both to determine which works better for thermal signature classification.\n",
    "\n",
    "Lets define a `create_model()` function which will instantiate our foundational ResNet-18 model given the `freeze_layers` parameter which will either freeze or unfreeze the backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(freeze_layers=True):\n",
    "    \"\"\"Create a CNN model using transfer learning.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    # Freeze all layers if required\n",
    "    if freeze_layers:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Modify the final layer for binary classification\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 2)  # 2 classes: human and animal\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model architecture summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## 7. Training Process\n",
    "\n",
    "We'll now define our training process. This includes setting up the loss function, optimiser, and learning rate scheduler, as well as functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimiser, scheduler=None, \n",
    "                num_epochs=10, patience=3):\n",
    "    \"\"\"Train the model and return training history.\"\"\"\n",
    "    # To store training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # For early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "        \n",
    "        history['val_loss'].append(epoch_loss)\n",
    "        history['val_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_loss < best_val_loss:\n",
    "            best_val_loss = epoch_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    # Load best model weights\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train our model using a learning rate of 0.001 over 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss function and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimiser, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "model, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimiser=optimiser,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=20,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualise how the model performed during training by plotting the train and validation set loss and accuracy scores against the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss vs. Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epochs')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots we see that the model's loss decreases and the accuracy increases for the validation set as we progress through the epochs which is a great sign as it indicates that our model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparameters\"></a>\n",
    "## 8. Hyperparameter Exploration\n",
    "\n",
    "Now, let's compare how different hyperparameters affect model performance. We'll explore two key hyperparameters: the learning rate and the convolutionsal layer weights.\n",
    "\n",
    "By looking at the configs array we have defined four different hyperparameter setups which we wll use to train four models which we will evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_hyperparams():\n",
    "    \"\"\"Run experiments with different hyperparameters.\"\"\"\n",
    "    # Define hyperparameter configurations to test\n",
    "    configs = [\n",
    "        {\n",
    "            'name': 'Frozen Backbone + High LR',\n",
    "            'freeze_layers': True,\n",
    "            'learning_rate': 0.001,\n",
    "            'color': 'blue'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Frozen Backbone + Low LR',\n",
    "            'freeze_layers': True,\n",
    "            'learning_rate': 0.0001,\n",
    "            'color': 'green'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Fine-tuned Backbone + Low LR',\n",
    "            'freeze_layers': False,\n",
    "            'learning_rate': 0.0001,\n",
    "            'color': 'red'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Fine-tuned Backbone + High LR',\n",
    "            'freeze_layers': False,\n",
    "            'learning_rate': 0.001,\n",
    "            'color': 'purple'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nTraining with configuration: {config['name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create model with specified freezing strategy\n",
    "        model = create_model(freeze_layers=config['freeze_layers'])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Set up optimiser with specified learning rate\n",
    "        optimiser = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimiser, step_size=7, gamma=0.1)\n",
    "        \n",
    "        # Train for a fixed number of epochs to enable fair comparison\n",
    "        model, history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=2,  # Fixed number of epochs for comparison\n",
    "            patience=100    # High patience to ensure all epochs run\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'history': history,\n",
    "            'model': model\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run hyperparameter experiments\n",
    "hyperparam_results = experiment_with_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparative results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot validation loss\n",
    "for result in hyperparam_results:\n",
    "    config = result['config']\n",
    "    history = result['history']\n",
    "    ax1.plot(history['val_loss'], label=config['name'], color=config['color'])\n",
    "\n",
    "ax1.set_title('Validation Loss vs. Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot validation accuracy\n",
    "for result in hyperparam_results:\n",
    "    config = result['config']\n",
    "    history = result['history']\n",
    "    ax2.plot(history['val_acc'], label=config['name'], color=config['color'])\n",
    "\n",
    "ax2.set_title('Validation Accuracy vs. Epochs')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these graphs it is evident that the model in which we use a low learning rate and fine tune the model layers results in a better model. This happens as when we keep the foundational model layers frozen it is using what it has learned from the RGB images it was trained on so it doesn't work so well with our thermal images which are not RGB. Thus by unfreezing these layers and training them on our thermal images with a slower learning rate the model learns to better handle our thermal images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for result in hyperparam_results:\n",
    "    config = result['config']\n",
    "    history = result['history']\n",
    "    \n",
    "    # Get final metrics\n",
    "    final_train_loss = history['train_loss'][-1]\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    final_train_acc = history['train_acc'][-1]\n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    \n",
    "    # Get best validation accuracy and its epoch\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    best_epoch = history['val_acc'].index(best_val_acc) + 1\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Configuration': config['name'],\n",
    "        'Learning Rate': config['learning_rate'],\n",
    "        'Freezing Strategy': 'Frozen Backbone' if config['freeze_layers'] else 'Fine-tuned Backbone',\n",
    "        'Final Train Loss': final_train_loss,\n",
    "        'Final Val Loss': final_val_loss,\n",
    "        'Final Train Acc': final_train_acc,\n",
    "        'Final Val Acc': final_val_acc,\n",
    "        'Best Val Acc': best_val_acc,\n",
    "        'Best Epoch': best_epoch\n",
    "    })\n",
    "\n",
    "# Create DataFrame and display\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df[['Configuration', 'Learning Rate', 'Freezing Strategy', \n",
    "                               'Final Train Loss', 'Final Val Loss', 'Final Train Acc', \n",
    "                               'Final Val Acc', 'Best Val Acc', 'Best Epoch']]\n",
    "\n",
    "# Format numeric columns\n",
    "for col in ['Final Train Loss', 'Final Val Loss', 'Final Train Acc', 'Final Val Acc', 'Best Val Acc']:\n",
    "    comparison_df[col] = comparison_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Hyperparameter Effects\n",
    "\n",
    "From our experiments, we can observe the impact of two key hyperparameter choices:\n",
    "\n",
    "1. **Model Freezing Strategy:**\n",
    "   - Freezing the backbone (using only pre-trained features) results in a model which has a higher validation set loss score and lower accuracy score as it does not cater to our \n",
    "   thermal signature images as well as fine-tuning the layers on ou thermal images. \n",
    "   - We can see that the freezing stratgey and learning rate go hand-in-hand and that different learning rates impact both freezing stratgeies differently.\n",
    "\n",
    "2. **Learning Rate:**\n",
    "   - A higher learning rate works best with the frozen backbone\n",
    "   - A lower learning rate works best with the fine tuned backbone as it can help prevent 'overstepping'\n",
    "\n",
    "Based on the results above, we'll continue with the best-performing configuration for our final evaluation, in this case the model that used a lower learning rate along with fine tuning the backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 9. Evaluation\n",
    "\n",
    "Now that we've identified the best hyperparameters, let's evaluate our model on the test set to assess its real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set and return predictions and metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    \n",
    "    return all_preds, all_labels, all_probs, accuracy\n",
    "\n",
    "# Select the best model from our hyperparameter experiments\n",
    "best_model_idx = np.argmax([result['history']['val_acc'][-1] for result in hyperparam_results])\n",
    "best_model = hyperparam_results[best_model_idx]['model']\n",
    "best_config = hyperparam_results[best_model_idx]['config']\n",
    "\n",
    "print(f\"Best configuration: {best_config['name']}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "preds, labels, probs, accuracy = evaluate_model(best_model, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "class_names = ['Animal', 'Human']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(labels, preds, target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analysing this confusion matrix we can see that our model classified our thermal signatures to a high degree of accuracy. While the model misclassified one thermal signature as an animal and 2 signatures as a human this level of accuracy can allow anti-poaching units to be efficiently sent out to assess a scene where a human is detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Predictions\n",
    "\n",
    "Let's visualise some of the predictions where the model classified correctly and incorrectly across both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_predictions(model, test_dataset, test_loader, num_examples=8):\n",
    "    \"\"\"Visualise model predictions on test data.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            batch_indices = list(range(i * test_loader.batch_size, \n",
    "                                      min((i + 1) * test_loader.batch_size, len(test_dataset))))\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_indices.extend(batch_indices)\n",
    "    \n",
    "    # Find examples of different prediction scenarios\n",
    "    correct_human = []\n",
    "    correct_animal = []\n",
    "    incorrect_human = []  # Actually human, predicted as animal\n",
    "    incorrect_animal = []  # Actually animal, predicted as human\n",
    "    \n",
    "    for idx, pred, label in zip(all_indices, all_preds, all_labels):\n",
    "        if pred == label == 1:  # Correct human\n",
    "            correct_human.append(idx)\n",
    "        elif pred == label == 0:  # Correct animal\n",
    "            correct_animal.append(idx)\n",
    "        elif pred == 0 and label == 1:  # Actually human, predicted as animal\n",
    "            incorrect_human.append(idx)\n",
    "        elif pred == 1 and label == 0:  # Actually animal, predicted as human\n",
    "            incorrect_animal.append(idx)\n",
    "    \n",
    "    # Randomly select from each category\n",
    "    example_indices = []\n",
    "    categories = [(correct_human, \"Correct Human\"), (correct_animal, \"Correct Animal\"), \n",
    "                  (incorrect_human, \"Incorrect Human\"), (incorrect_animal, \"Incorrect Animal\")]\n",
    "    \n",
    "    for category, label in categories:\n",
    "        if category:\n",
    "            selected = random.sample(category, min(num_examples // 4, len(category)))\n",
    "            example_indices.extend([(idx, label) for idx in selected])\n",
    "    \n",
    "    # Calculate proper grid dimensions\n",
    "    total_examples = len(example_indices)\n",
    "    if total_examples == 0:\n",
    "        print(\"No examples to display!\")\n",
    "        return\n",
    "        \n",
    "    # Decide on a reasonable grid layout\n",
    "    if total_examples <= 4:\n",
    "        n_cols = total_examples\n",
    "        n_rows = 1\n",
    "    else:\n",
    "        n_cols = 4  # Fixed number of columns\n",
    "        n_rows = (total_examples + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    # Visualise selected examples\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    for i, (idx, category) in enumerate(example_indices):\n",
    "        if i >= n_rows * n_cols:  # Safety check\n",
    "            break\n",
    "            \n",
    "        img, label = test_dataset[idx]\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            input_tensor = img.unsqueeze(0).to(device)\n",
    "            output = model(input_tensor)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            prob = F.softmax(output, dim=1)[0, pred].item()\n",
    "        \n",
    "        # Convert image for display (take first channel since we repeated grayscale)\n",
    "        img_display = img[0].numpy()\n",
    "        \n",
    "        # Denormalise\n",
    "        img_display = img_display * 0.229 + 0.485\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "        ax.imshow(img_display, cmap='gray')\n",
    "        ax.set_title(f\"{category}\\nTrue: {class_names[label]} \\nPred: {class_names[pred]}\\nConf: {prob:.2f}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualise predictions\n",
    "visualise_predictions(best_model, test_dataset, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key visualise difference between animal and human thermal signatures is the shape of the thermal signatures. Human thermal signatures will tend to be more skinny and vertical as supposed to four legged animals. If an animal in the image has a more vertical signature the model can misclasify it as a human and vice versa when a human bends down or creates a more horizontal signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## 10. Results and Discussion\n",
    "\n",
    "Let's summarise our findings and discuss the implications for wildlife conservation applications.\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "Our fine tuned CNN model deomstrates strong accuracy for our problem of classifying human and animal thermal signatures. Key findings include:\n",
    "\n",
    "1. **Overall Performance**: The model achieved a high accuracy score of `0.9779` on the test set.\n",
    "\n",
    "2. **Hyperparameter Effects**:\n",
    "   - Fine-tuning the entire network with a lower learning rate yielded better results than freezing the backbone with a higher learning rate.\n",
    "   - This suggests that the convolutional layers can adapt to ou thermal imagery despite being pre-trained on RGB images.\n",
    "\n",
    "3. **Class Performance**:\n",
    "   - Human detection showed higher precision, indicating fewer false positives which is hugely beneficial for anti-poaching resource allocation.\n",
    "\n",
    "4. **Error Analysis**:\n",
    "   - Some animal signatures with human-like postures (standing animals) were confused with humans.\n",
    "\n",
    "### Applications and Limitations\n",
    "\n",
    "This model could be integrated into drone-based anti-poaching systems with the following considerations:\n",
    "\n",
    "- **Real-time Analysis**: The lightweight ResNet-18 architecture allows for efficient deployment on edge devices attached to drones.\n",
    "- **Alert System**: High confidence human detections could trigger alerts for ranger teams, focusing their attention on potential poaching activities.\n",
    "- **Limitations**: Performance may vary in different geographic regions with different wildlife.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Data Augmentation**: More sophisticated thermal-specific augmentations to improve model robustness.\n",
    "3. **Model Architecture**: Exploring architectures specifically designed for thermal imagery rather than adapted from RGB domains.\n",
    "4. **Image Processing**: Advanced image processing techniques could be used to increase the quality of the thermal signaures and in turn increasing the models accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "## 11. References\n",
    "\n",
    "1. pytorch.org. (n.d.). Transfer Learning for Computer Vision Tutorial — PyTorch Tutorials 1.7.0 documentation. [online] Available at: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html.\n",
    "\n",
    "2. Team, K. (2020). Keras documentation: Image classification via fine-tuning with EfficientNet. [online] keras.io. Available at: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/.\n",
    "\n",
    "3. Team, K. (2020). Keras documentation: Transfer learning & fine-tuning. [online] keras.io. Available at: https://keras.io/guides/transfer_learning/.\n",
    "\n",
    "4. Mills, C. (2023). Fine-Tuning Image Classifiers with PyTorch and the timm library for Beginners – Christian Mills. [online] Christian Mills. Available at: https://christianjmills.com/posts/pytorch-train-image-classifier-timm-hf-tutorial/#selecting-a-model [Accessed 8 Mar. 2025].\n",
    "\n",
    "5. Microavia (2024). How drones are used in wildlife monitoring to protect against poaching. [online] Microavia.com. Available at: https://microavia.com/news/unmanned_aerial_vehicles_for_wildlife_protection.\n",
    "\n",
    "6. Harvard University (2020). Conservation Drones. [online] LILA BC. Available at: https://lila.science/datasets/conservationdrones [Accessed 8 Mar. 2025]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
